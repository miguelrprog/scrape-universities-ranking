{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/an-introduction-to-web-scraping-with-python-a2601e8619e5\n",
    "\n",
    "\n",
    "http://www.webometrics.info/es/americas/latin_america\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.webometrics.info/es/americas/latin_america'\n",
    "r = requests.get(url)\n",
    "html_contents = r.text\n",
    "html_soup = BeautifulSoup(html_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.webometrics.info/es/americas/latin_america?page=0',\n",
       " 'http://www.webometrics.info/es/americas/latin_america?page=1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_url = \"http://www.webometrics.info/es/americas/latin_america?page={}\"\n",
    "last_page = html_soup.find('li', class_='pager-last').a['href'].split('=')[1]\n",
    "pages_urls = [page_url.format(i) for i in range(0, int(last_page) + 1)]\n",
    "pages_urls[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to store the contents of each url to be scraped\n",
    "def getAndParseURL(url):\n",
    "    r = requests.get(url)\n",
    "    html_soup = BeautifulSoup(r.text)\n",
    "    return(html_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = []\n",
    "country = []\n",
    "# change to pages_urls[:] to scrape all pages (40).\n",
    "for url in pages_urls[:2]:\n",
    "    html_soup = getAndParseURL(url)\n",
    "    tables = html_soup.find('table', class_='sticky-enabled')\n",
    "    for c in tables.findAll(\"img\", src=re.compile(\"logos/\")):\n",
    "        country_symbol = c.get('src').split('logos/')[1].split('.')[0]\n",
    "        country.append(country_symbol)\n",
    "    for item in tables:\n",
    "        headers = []\n",
    "        rows = tables.find_all('tr')\n",
    "        for header in tables.find('tr').find_all('th'):\n",
    "            headers.append(header.text)\n",
    "    for row in tables.find_all('tr'):\n",
    "        values = []\n",
    "        for col in row.find_all('td'):\n",
    "            values.append(col.text)\n",
    "        e_dict = {headers[i]: values[i] for i in range(len(values))} \n",
    "        ranking.append(e_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the ranking\n",
    "df = pd.DataFrame(ranking)\n",
    "df_clean = df.dropna(how='all') # clean null values\n",
    "df_clean['País'] = country # replace País(Country) column with country abbreviation\n",
    "df_clean.to_csv('ranking_universities_latin_america.csv') # export to a csv file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
